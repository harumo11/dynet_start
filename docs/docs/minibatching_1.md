# ミニバッチ

## ミニバッチの概要

ミニバッチ処理では，複数のトレーニングデータを取り上げ，それらをグループ化して同時に処理するものです．
これは現代的なハードウェア(GPUだけでなくCPUも)が適切に構造化された
入力で活用できるベクトル演算命令を持ち合わせているために
計算効率の面で大きな効果を上げることができるためです.
以下の図で示すような，
一つ一つのベクトルを個別に計算するのとは対象的に，同時に複数のベクトルにまたがって
要素単位の処理を実行したり，複数の教師データを１つの行列x行列の掛け算になるように表現し処理したり，
また，複数の行列xベクトル計算をグループ化したりするような計算を含むネットワーク
が一般的な例として挙げられます．

殆どのニューラルネットのツールキットでは，ミニバッチ処理の殆どをユーザが
ツールキットの助けを借りて書く必要がありました．
これは通常，処理に関係があるテンソルに追加の次元を加え，全ての操作が処理を
実行するときにこの次元を考慮するようにすることで行われます．
これは，ユーザの負担になります．ユーザはこの追加のバッチの次元を記憶にとどめながら
全ての計算処理を書かなくてはいけなくなります．そして，最大の計算効率を得るために
適切なバッチの次元を必ず使用しなければなりません．
さらにユーザはバッチされた要素とされていない要素を組み合せた処理(ニューラルネットの
バッチ処理された隠れ層の状態とバッチされていないパラメータ行列，もしくはベクトルなど)をするときに
多大な注意を払わなければなりません．その場合，ベクトルをバッチに連結するか，
バッチ処理された要素に「ブロードキャスト」して，バッチ次元に沿って複製し，不正な
次元の不一致がないようにする必要があります．

DyNetは完全に自動化できる使いやすいミニバッチ機能により，この複雑さの殆どをユーザから隠します．

## 自動ミニバッチ機能

もし，あなたが何もすることなしに多くのミニバッチの恩恵を受けたいのであれば，
DyNetの自動ミニバッチ機能を使ってください．

この機能は`--dynet-autobach 1`をコマンドラインオプションで渡すことで有効になります．
そして，この機能が有効になるとDyNetは効率を上げるために,
まとめてバッチ処理できる操作を自動で見つけようと試みます．
これを最大限利用するために，あなたは単にたくさんのトレーニングサンプルを繰り返すだけで
その巨大なトレーニングサンプルを表現できるような
巨大なcomputation graphを作りたくなるでしょう．
例えば，下記のような

```Python
% minibatchをtraining_dataから取り出す
for minibatch in training_data:
	dy.renew_cg()	% computation graphを新しく用意する
	losses = []

	% ミニバッチからデータポイントを取り出す
	for x, y, in minibatch:	
		l = calculate_my_loss(x,y)	% 損失を計算する
		losses.append(l)			% 損失を保存する
	loss = dy.esum(losses)			% 損失の合計を計算する
	loss.forward()
	loss.backward()
	trainer.update()
```

`calculate_my_function`は任意の複雑性を有し
プログラム全体で同じ構造を持つ必要がないためこれは素晴らしいことです．
木構造を有するニューラルネットのモデルに対する実行可能な完全な例は[ここ](https://github.com/neulab/dynet-benchmark/blob/master/dynet-cpp/treenn-bulk.cc)
にあります．

## 手動ミニバッチ機能

プログラム全体に渡って同じ構造を保持している簡単にミニバッチできるネットワークにおいて
手動でミニバッチ処理を行うことで,他のツールキットで行うことと同様に
より一層の利益を得ることが可能です．
このような場合でも，DyNetは，多くのミニバッチ要素を別の標準次元としてではなく，
特定の意味論を持つ特別な次元として扱う特別に設計されたバッチ操作を使用することで
ユーザからこの複雑さの多くを隠します．
DyNetに実装された各操作ごとにブロードキャスティングが舞台裏で行われ，
そして，ユーザは各バッチごとの複数のデータの入力だけを考えるだけ良くなっており,
さらに，複数のラベルを用いて損失が計算されます．

まずはじめに，ミニバッチ処理されていないPythonのサンプルを見てみましょう．
この例では，我々は単語の埋め込み`word_1`と`word_2`をルックアップパラメータ`E`を用いて
検索します．そして，重み`W`とバイアス`b`を用いてアフィン変換を実施し，そしてソフトマックス
を実施します．最後に，正解ラベル`out_label`によって損失を計算します．

```Python
# in_words is a tuple (word_1, word_2)
# out_label is an output label
word_1 = E[in_words[0]]
word_2 = E[in_words[1]]
scores_sym = W*dy.concatenate([word_1, word_2])+b
loss_sym = dy.pickneglogsoftmax(socores_sym, out_label)
```

次に，ミニバッチバージョンを見てみましょう

```Python
# in_words is a list [(word_{1,1}, word_{1,2}), (word_{2,1}, word_{2,2}, ...]
# out_label is a list of output labels [label_1, label_2, ...]
word_1_batch = dy.lookup_batch(E, [x[0] for x in in_words])
word_2_batch = dy.lookup_batch(E, [x[0] for x in in_words])
scores_sym = W*dy.concatenate([word_1_batch, word_2_batch])+b
loss_sym = dy.sum_batches(dy.pickneglogsoftmax_batch(scores_sym, out_labels))
```

４つの大きな変更があります：word IDは１つのIDではなくIDのリストに変更が必要です．
`lookup_batch`を普通のlookupの代わりに呼ぶ必要があります．`pickneglogsoftmax_batch`を
普通のpickneglogsoftmaxの代わりに呼ぶ必要があります．そして，最後に`sum_batch`を呼んで
全てのバッチからの損失の合計を計算する必要があります．

ミニバッチを利用した言語処理のためのRNNの完全な例は[ここ]()で見つかります．

## ミニバッチの次元

DyNetがこれを処理する方法は特別な特権を有する**mini-batch element**次元を用いることです．
**mini-batch element**次元はミニバッチ中のトレーニングサンプル数を指し示します．
C++の例を上げると，我々は`Dim`オブジェクトを宣言します．

```C++
Dim d({2,4,8}, 16)
```

もしくは，Pythonでは

```Python
d = Dim([2,4,8], 16)
```

ここで，`2,4,8`は各トレーニングサンプルに対するテンソル中のデータの次元です．
`16`はミニバッチにサンプルの数です．
我々が次元を表示させるとき(例えば，`print_graphviz`をデバッグのために呼んだときなど)
これは`{2,4,8x16}`と表示されるでしょう．

## ミニバッチに対応した機能

多くの大多数の普通の機能に対して，物事はシームレスにミニバッチに関しても動くべきです．
唯一の条件は全ての入力が１つのミニバッチ要素のみを有しているか，もしくは
ミニバッチ要素の数が同じかのいずれかです．
なので，2つの引数を受け取る関数`f(x,y)`においては，ミニバッチ要素の数`x/y`はそれぞれ`1/1`, `4/1`, `1/4`,もしくは
`4/4`です．
しかしながら，異なる１つではないミニバッチ要素の数を有することはできません．
たとえば，`x/y`が`2/4`のみにバッチサイズを有するなどです．

我々が陽にバッチについて考える必要があるいくつかの操作があります．
殆どはcomputation graphの入出力の側ですが．
それらを含んだ入力操作です．

- `lookup()`(C++)と`lookup_batch()`(Python):入力IDsのベクトルを検索します．ここでの各入力IDはミニバッチの要素です．
- `input()`(C++) 入力はミニバッチ処理された`Dim`オブジェクトによって指定できます．

損失関数処理

- `pickneglogsoftmax()`: negative log softmax損失カンスを複数のバッチ要素に関して計算します．
- `hinge()` : 同様に，ヒンジ損失関数を複数の要素に渡って計算します．

操作

- `reshape()` : １つ以上のバッチ要素とともにテンソルを変形するときに使用できます
- `pick()` : それぞれのミニバッチ要素を取り出します．
- `sum_batches()` 関数はバッチの中の全ての値の合計を計算します．この関数はよく逆伝播を行う前に損失関数の合計を計算するために実行されます．
