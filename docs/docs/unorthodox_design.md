# Unorthodox Design

!!! note "unorthodox"
	非正統派という意味．よってunotrhodox designは非正統的なデザインと訳される

いくつかの箇所で他のライブラリの実装とは異なるをdynetは選択しました．
もしくは，あなたが想定している実装方法とは幾分異なる箇所があるかもしれません．
下記に非正統的なデザインを選択した箇所のリストを示します．
これを読むことにより，プログラムを書いているときに驚くのを避けることがきます.

## Sparse Update

!!! note
	Sparse Update = 疎な更新

デフォルトでは，dynetのパラメータオプティマイザは`LookupParameters`を通して
疎な更新を実行します．
つまり，この意味は`LookupParameters`オブジェクトがあり，特定のインデックスの
サブセットを使用してからパラメータの更新を実行すると，オプティマイザは
使用済みサブセットをループし，未使用の値を更新することはありません．
これはいくつかの場合において効率を改善することができます．
あなたが10万語の語彙のための埋め込みを持っていて，あなたご特定の更新で
それらのうち５つだけを使うならば，これにより全ての10万語以上の更新を
避けることができるでしょう．ただし，注意することが２つあります．
１つめは，`MomentumSDGTrainer`と`AdamTrainer`のようなモーメンタムを使用している
更新則は厳密に正しいものではなくなります．(いくつかの努力により正しくすることもできます
が，それはプログラミングを複雑にします．我々はそれを選択しませんでした．)

## Dropout Scaling

オーバーフィッティングを避けるためにドロップアウトは使用されます．ドロップアウトは一般的に
トレーニング時においてロバスト性を上げるために使用されます．
そして，推論時にはニューラルネットの全てのノードが最終決定に使用されます．
しかし，各状況で使用されているノードの数の間には隔たりがあるため，出力の値をスケーリング
して学習と推論時で一致させるようにすることが重要です．これには２つの方法があります．

- Vanilla Dropout:
	トレーニング時において，確率`p`でドロップアウトを行います．推論時にはそれぞれのノードの出力を`p`倍します. 
- Inverted Dropout:
	トレーニング時に確率`p`でドロップアウトを行います．そして出力を`1/p`倍します．推論時には出力をそのまま使用します．
	もし，`p=1`を選択すれば常にドロップアウトされます．そうでなくて，`p=0`を選択すれば
	全てのノードがそのままになります．

前者はより一般的なものです．後者はより便利なものです．なぜなら我々はドロップアウトを
トレーニング時にのみ考えれば良くなるからです．そしてDyNetは後者を選択しました．
